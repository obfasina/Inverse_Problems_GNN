Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83769.0979968767  Validation Loss: 81793.5832217403
Epoch: 20  Train Loss: 82395.79917467434  Validation Loss: 78493.71065764074
Epoch: 30  Train Loss: 79592.08097995381  Validation Loss: 73701.69175113789
Epoch: 40  Train Loss: 74677.01801542273  Validation Loss: 67114.57239509236
Epoch: 50  Train Loss: 67213.08905272589  Validation Loss: 57108.25563634125
Epoch: 60  Train Loss: 57313.333809424104  Validation Loss: 48025.942418263665
Epoch: 70  Train Loss: 46244.64607869872  Validation Loss: 43931.94942821897
Epoch: 80  Train Loss: 35244.69913130822  Validation Loss: 43956.0666799248
Epoch: 90  Train Loss: 26969.867117243393  Validation Loss: 46049.75936412649
Epoch: 100  Train Loss: 18293.593784140685  Validation Loss: 46046.65826517099
Epoch: 110  Train Loss: 11949.888082763735  Validation Loss: 41851.00984449199
Epoch: 120  Train Loss: 6827.784283457312  Validation Loss: 33569.237468466396
Epoch: 130  Train Loss: 4434.392639363693  Validation Loss: 33480.89232039899
Epoch: 140  Train Loss: 6614.670978840916  Validation Loss: 25487.629796437443
Epoch: 150  Train Loss: 4108.492696110483  Validation Loss: 19524.117625266463
Epoch: 160  Train Loss: 2440.050497345623  Validation Loss: 19861.590323052627
Epoch: 170  Train Loss: 7566.437751161291  Validation Loss: 35009.24619844275
Epoch: 180  Train Loss: 4750.793677593454  Validation Loss: 17511.290497416787
Epoch: 190  Train Loss: 2904.264851594536  Validation Loss: 11464.581499367474
Epoch: 200  Train Loss: 1961.1964254310813  Validation Loss: 7574.888609920449
Epoch: 210  Train Loss: 1530.7426699474909  Validation Loss: 6897.026690590759
Epoch: 220  Train Loss: 1378.9890304312278  Validation Loss: 7763.428455448513
Epoch: 230  Train Loss: 1733.389646145498  Validation Loss: 4996.836232740927
Epoch: 240  Train Loss: 6880.929385688328  Validation Loss: 12010.719001186626
Epoch: 250  Train Loss: 2136.5375636156505  Validation Loss: 4259.99619694098
Epoch: 260  Train Loss: 1122.969622646634  Validation Loss: 3674.914074634515
Epoch: 270  Train Loss: 1069.575978523117  Validation Loss: 3388.160160768625
Epoch: 280  Train Loss: 891.5707664804655  Validation Loss: 3244.0885978989104
Epoch: 290  Train Loss: 856.912928766471  Validation Loss: 3126.8963894912226
Epoch: 300  Train Loss: 885.1635129462051  Validation Loss: 3290.9658986479394
Epoch: 310  Train Loss: 1227.7692741552444  Validation Loss: 4442.163889621464
Epoch: 320  Train Loss: 855.0384760515227  Validation Loss: 2834.494477059542
Epoch: 330  Train Loss: 923.6942227245332  Validation Loss: 3457.153877970713
Epoch: 340  Train Loss: 1365.0180643885876  Validation Loss: 9355.542952014803
Epoch: 350  Train Loss: 1465.7516780191534  Validation Loss: 3822.4983486667984
Epoch: 360  Train Loss: 1009.2679345111492  Validation Loss: 2982.100536770759
Epoch: 370  Train Loss: 805.5453155733949  Validation Loss: 4267.279994413409
Epoch: 380  Train Loss: 792.5307090453483  Validation Loss: 2651.9810172260245
Epoch: 390  Train Loss: 948.2808581592617  Validation Loss: 2864.897345922647
Epoch: 400  Train Loss: 938.875112038385  Validation Loss: 2459.5553925232985
Epoch: 410  Train Loss: 688.4517030132359  Validation Loss: 2356.4038759176847
Epoch: 420  Train Loss: 752.4800390126743  Validation Loss: 2269.142396898705
Epoch: 430  Train Loss: 684.2175013059355  Validation Loss: 2315.4009062032114
Epoch: 440  Train Loss: 1049.5508064121152  Validation Loss: 3538.472234639443
Epoch: 450  Train Loss: 737.1567595497091  Validation Loss: 2382.663701741115
Epoch: 460  Train Loss: 599.3742535149332  Validation Loss: 2182.9825084154218
Epoch: 470  Train Loss: 917.9536477891775  Validation Loss: 2448.0623460082857
Epoch: 480  Train Loss: 571.2304237289237  Validation Loss: 2062.4483699500784
Epoch: 490  Train Loss: 929.2751570186683  Validation Loss: 2215.2360906988906
Epoch: 500  Train Loss: 623.3003947035023  Validation Loss: 2228.8996101780353
Epoch: 510  Train Loss: 851.0700048307837  Validation Loss: 2013.4534021663003
Epoch: 520  Train Loss: 918.662853388017  Validation Loss: 2647.205733983044
Epoch: 530  Train Loss: 635.7510163505432  Validation Loss: 2031.3888113900564
Epoch: 540  Train Loss: 616.9054693369886  Validation Loss: 1882.9410376182975
Epoch: 550  Train Loss: 579.3254050744451  Validation Loss: 1830.7480678461852
Epoch: 560  Train Loss: 599.4726820950899  Validation Loss: 1982.7835318487616
Epoch: 570  Train Loss: 916.0122427697958  Validation Loss: 1784.0860569870479
Epoch: 580  Train Loss: 711.931499532042  Validation Loss: 1758.3936332515889
Epoch: 590  Train Loss: 831.087689022692  Validation Loss: 1860.0453784559318
Epoch: 600  Train Loss: 698.27242005058  Validation Loss: 1812.0326628142252
Epoch: 610  Train Loss: 588.9852311282432  Validation Loss: 1588.1229687644775
Epoch: 620  Train Loss: 529.5038678177458  Validation Loss: 1747.9483785480197
Epoch: 630  Train Loss: 1030.9093220645912  Validation Loss: 3099.4833157538064
Epoch: 640  Train Loss: 592.1243127174156  Validation Loss: 1841.6088143365077
Epoch: 650  Train Loss: 535.0125309900009  Validation Loss: 1511.3360977244163
Epoch: 660  Train Loss: 635.0064498498596  Validation Loss: 1395.0997021669389
Epoch: 670  Train Loss: 659.0792358873846  Validation Loss: 2990.5799535785536
Epoch: 680  Train Loss: 644.1174783462046  Validation Loss: 1854.113747278818
Epoch: 690  Train Loss: 515.7597541257322  Validation Loss: 1404.5804553735582
Epoch: 700  Train Loss: 781.0498075421584  Validation Loss: 1409.2607246342548
Epoch: 710  Train Loss: 827.1296572383663  Validation Loss: 2634.1743338320994
Epoch: 720  Train Loss: 534.8466655444673  Validation Loss: 1549.3396620551719
Epoch: 730  Train Loss: 611.0673311790554  Validation Loss: 1253.4789436564226
Epoch: 740  Train Loss: 469.5448486934872  Validation Loss: 1374.9633396849417
Epoch: 750  Train Loss: 1020.0069917464483  Validation Loss: 1590.5701184581376
Epoch: 760  Train Loss: 513.1746916996084  Validation Loss: 1267.3254564205606
Epoch: 770  Train Loss: 565.8232211358066  Validation Loss: 1226.058443951139
Epoch: 780  Train Loss: 605.9165186925188  Validation Loss: 1441.216392897405
Epoch: 790  Train Loss: 569.4919378203937  Validation Loss: 1396.0809581654537
Epoch: 800  Train Loss: 530.1409378425254  Validation Loss: 1209.2355008371783
Epoch: 810  Train Loss: 1733.3078248123395  Validation Loss: 1446.7908075827659
Epoch: 820  Train Loss: 569.9582791994864  Validation Loss: 1133.2673516971504
Epoch: 830  Train Loss: 1176.869467971908  Validation Loss: 5084.948057788359
Epoch: 840  Train Loss: 555.5061166569403  Validation Loss: 1327.5220482089503
Epoch: 850  Train Loss: 401.79788429386207  Validation Loss: 1064.0985244498145
Epoch: 860  Train Loss: 384.3432150159504  Validation Loss: 1041.7382999283116
Epoch: 870  Train Loss: 824.1634727249566  Validation Loss: 2932.3493505260853
Epoch: 880  Train Loss: 492.87807214708937  Validation Loss: 1112.9483727493296
Epoch: 890  Train Loss: 396.61055885141474  Validation Loss: 1060.166963079334
Epoch: 900  Train Loss: 427.36972398147617  Validation Loss: 1135.3995739936906
Epoch: 910  Train Loss: 604.9988121858356  Validation Loss: 1613.8689602154348
Epoch: 920  Train Loss: 524.7127426498694  Validation Loss: 960.0151748128914
Epoch: 930  Train Loss: 642.3080806827336  Validation Loss: 1732.201051686746
Epoch: 940  Train Loss: 810.6875620927352  Validation Loss: 4141.548172418494
Epoch: 950  Train Loss: 703.6958117243198  Validation Loss: 1203.5958438037717
Epoch: 960  Train Loss: 695.7536781621262  Validation Loss: 2192.26486766885
Epoch: 970  Train Loss: 497.03917559730286  Validation Loss: 1210.0523127772867
Epoch: 980  Train Loss: 657.4703381525018  Validation Loss: 1626.3941669896674
Epoch: 990  Train Loss: 786.2017912746014  Validation Loss: 2643.0302924659386
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(1115.8396, device='cuda:0', grad_fn=<DivBackward0>)