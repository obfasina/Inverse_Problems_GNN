Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83921.65912902572  Validation Loss: 82236.39321601437
Epoch: 20  Train Loss: 82870.57869647075  Validation Loss: 79682.9571479496
Epoch: 30  Train Loss: 80824.0939150296  Validation Loss: 76037.56278244851
Epoch: 40  Train Loss: 77161.06401131932  Validation Loss: 70364.11385480702
Epoch: 50  Train Loss: 71467.82523483476  Validation Loss: 61462.696237599186
Epoch: 60  Train Loss: 63412.24783753458  Validation Loss: 53416.195810470555
Epoch: 70  Train Loss: 52830.36562952582  Validation Loss: 46520.82217373852
Epoch: 80  Train Loss: 42004.74788250849  Validation Loss: 43883.463809775305
Epoch: 90  Train Loss: 31105.855744789285  Validation Loss: 45353.31587781316
Epoch: 100  Train Loss: 21697.432165630697  Validation Loss: 47548.774033703914
Epoch: 110  Train Loss: 16085.230339550331  Validation Loss: 46746.18367253985
Epoch: 120  Train Loss: 8619.025574157185  Validation Loss: 43178.7144065478
Epoch: 130  Train Loss: 4324.532004983477  Validation Loss: 38127.570101019795
Epoch: 140  Train Loss: 2836.35985378097  Validation Loss: 32826.47082062824
Epoch: 150  Train Loss: 1978.140687932894  Validation Loss: 28386.283714672867
Epoch: 160  Train Loss: 1788.0353217237064  Validation Loss: 29203.804865165082
Epoch: 170  Train Loss: 1898.361992825543  Validation Loss: 18655.31819825767
Epoch: 180  Train Loss: 1281.0004673293815  Validation Loss: 22723.060406872446
Epoch: 190  Train Loss: 1455.802984824444  Validation Loss: 33174.45335451905
Epoch: 200  Train Loss: 1687.2366837760708  Validation Loss: 18857.42663315377
Epoch: 210  Train Loss: 1229.5762485123116  Validation Loss: 15037.91027785306
Epoch: 220  Train Loss: 1092.2570750094385  Validation Loss: 17512.37464247872
Epoch: 230  Train Loss: 1303.8334806420878  Validation Loss: 19395.12043349703
Epoch: 240  Train Loss: 1317.8830331527263  Validation Loss: 13663.327655784373
Epoch: 250  Train Loss: 996.672545629083  Validation Loss: 15797.6455557914
Epoch: 260  Train Loss: 969.547261078215  Validation Loss: 13561.754661763072
Epoch: 270  Train Loss: 1246.4966947552148  Validation Loss: 16406.93100682287
Epoch: 280  Train Loss: 1016.3679747157986  Validation Loss: 13093.398382348076
Epoch: 290  Train Loss: 1790.1202819972284  Validation Loss: 16546.529236845014
Epoch: 300  Train Loss: 983.132983463281  Validation Loss: 12282.983665160084
Epoch: 310  Train Loss: 1196.8449832402891  Validation Loss: 12249.159135214704
Epoch: 320  Train Loss: 965.4974226247263  Validation Loss: 11763.715677094504
Epoch: 330  Train Loss: 1167.0995985855236  Validation Loss: 11541.795786026501
Epoch: 340  Train Loss: 809.3244598766322  Validation Loss: 11442.030235484217
Epoch: 350  Train Loss: 1109.1290775181562  Validation Loss: 11013.722300549363
Epoch: 360  Train Loss: 815.9751793743499  Validation Loss: 10812.738472806024
Epoch: 370  Train Loss: 1404.469457185285  Validation Loss: 10629.98472049584
Epoch: 380  Train Loss: 885.330403686954  Validation Loss: 10376.411972501406
Epoch: 390  Train Loss: 632.3005500107546  Validation Loss: 10160.243591711314
Epoch: 400  Train Loss: 777.6883526943701  Validation Loss: 9922.793551484026
Epoch: 410  Train Loss: 691.1669256064857  Validation Loss: 9702.158371329606
Epoch: 420  Train Loss: 1897.636189781333  Validation Loss: 9996.638348958113
Epoch: 430  Train Loss: 643.0726415716348  Validation Loss: 9404.730159157232
Epoch: 440  Train Loss: 616.5886754921993  Validation Loss: 9378.86902879385
Epoch: 450  Train Loss: 589.0074077316226  Validation Loss: 9770.379315660126
Epoch: 460  Train Loss: 504.0510690604721  Validation Loss: 9297.037950594955
Epoch: 470  Train Loss: 768.0292331309038  Validation Loss: 8948.223530923757
Epoch: 480  Train Loss: 1171.796073036563  Validation Loss: 11008.557885497383
Epoch: 490  Train Loss: 1545.7778806253143  Validation Loss: 8283.37196997042
Epoch: 500  Train Loss: 1144.1361391613248  Validation Loss: 6647.645967337077
Epoch: 510  Train Loss: 1025.7548814898844  Validation Loss: 5468.3330045299435
Epoch: 520  Train Loss: 1242.6394164971564  Validation Loss: 5461.925967612373
Epoch: 530  Train Loss: 913.00930665095  Validation Loss: 4806.331067378042
Epoch: 540  Train Loss: 753.0649617054943  Validation Loss: 4755.857057963994
Epoch: 550  Train Loss: 798.6705008931139  Validation Loss: 8027.571588858906
Epoch: 560  Train Loss: 760.989743607702  Validation Loss: 3367.039842406342
Epoch: 570  Train Loss: 750.8133056382508  Validation Loss: 3718.2289632002908
Epoch: 580  Train Loss: 652.673646643759  Validation Loss: 2947.0267544717362
Epoch: 590  Train Loss: 697.7745851348183  Validation Loss: 5845.215091840206
Epoch: 600  Train Loss: 416.6019120898915  Validation Loss: 3012.8447275350572
Epoch: 610  Train Loss: 591.0254082879369  Validation Loss: 3935.7595629626617
Epoch: 620  Train Loss: 2121.633964332462  Validation Loss: 6333.427649637139
Epoch: 630  Train Loss: 489.9088074236868  Validation Loss: 2573.2111885062054
Epoch: 640  Train Loss: 847.2695678361082  Validation Loss: 4325.468912726996
Epoch: 650  Train Loss: 421.6839016113522  Validation Loss: 2778.452800227065
Epoch: 660  Train Loss: 443.4144148179749  Validation Loss: 2329.469375498364
Epoch: 670  Train Loss: 447.11485655984853  Validation Loss: 3584.7279016767093
Epoch: 680  Train Loss: 405.07705503158417  Validation Loss: 2946.1967331393766
Epoch: 690  Train Loss: 570.3237210550018  Validation Loss: 3087.100722087309
Epoch: 700  Train Loss: 1106.0789495044846  Validation Loss: 7349.154242263338
Epoch: 710  Train Loss: 530.7819772103906  Validation Loss: 3033.1800681080294
Epoch: 720  Train Loss: 620.8152452218819  Validation Loss: 2395.438912706365
Epoch: 730  Train Loss: 324.8346560049531  Validation Loss: 2345.381985807394
Epoch: 740  Train Loss: 353.41688076556557  Validation Loss: 2053.9679938523514
Epoch: 750  Train Loss: 521.6859640796671  Validation Loss: 2166.7472667967336
Epoch: 760  Train Loss: 413.22849163911883  Validation Loss: 2779.442187962645
Epoch: 770  Train Loss: 390.121923153865  Validation Loss: 1983.1038697216597
Epoch: 780  Train Loss: 523.412005468889  Validation Loss: 3540.5813941736096
Epoch: 790  Train Loss: 687.955687588164  Validation Loss: 2096.4508491151496
Epoch: 800  Train Loss: 289.5973182476153  Validation Loss: 2701.991582024765
Epoch: 810  Train Loss: 379.03208048792754  Validation Loss: 1821.7654215054465
Epoch: 820  Train Loss: 707.9598999992828  Validation Loss: 3402.7214753855765
Epoch: 830  Train Loss: 355.2976179327613  Validation Loss: 1777.338963909456
Epoch: 840  Train Loss: 237.03033774708044  Validation Loss: 2504.0649973466348
Epoch: 850  Train Loss: 466.7118437574509  Validation Loss: 2621.2691325031974
Epoch: 860  Train Loss: 443.21418458430315  Validation Loss: 2149.3830641618815
Epoch: 870  Train Loss: 442.33758214203635  Validation Loss: 2522.7092728335615
Epoch: 880  Train Loss: 608.2740231679668  Validation Loss: 2340.826727447502
Epoch: 890  Train Loss: 278.9256917812042  Validation Loss: 2083.463776193542
Epoch: 900  Train Loss: 722.104163060851  Validation Loss: 3273.051339040953
Epoch: 910  Train Loss: 422.05438192274624  Validation Loss: 2001.1680493163942
Epoch: 920  Train Loss: 395.29043048171053  Validation Loss: 2554.6886589696333
Epoch: 930  Train Loss: 341.17754577223195  Validation Loss: 1686.8100634471991
Epoch: 940  Train Loss: 384.3646082577018  Validation Loss: 1723.9791898729225
Epoch: 950  Train Loss: 818.0786461727624  Validation Loss: 3317.8379986021864
Epoch: 960  Train Loss: 810.4493393086964  Validation Loss: 5474.423843136924
Epoch: 970  Train Loss: 342.70697052857287  Validation Loss: 2008.771456841627
Epoch: 980  Train Loss: 1007.4148395571083  Validation Loss: 3835.930477125361
Epoch: 990  Train Loss: 424.2741405410041  Validation Loss: 1718.509965686017
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(1372.5341, device='cuda:0', grad_fn=<DivBackward0>)