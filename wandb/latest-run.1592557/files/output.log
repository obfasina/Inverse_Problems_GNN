Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83708.80299777925  Validation Loss: 81685.82452027695
Epoch: 20  Train Loss: 82524.94764203865  Validation Loss: 78982.44742854004
Epoch: 30  Train Loss: 79883.64492972205  Validation Loss: 74894.34805332682
Epoch: 40  Train Loss: 75251.10750772612  Validation Loss: 67896.36594655078
Epoch: 50  Train Loss: 68020.824052673  Validation Loss: 59067.96738182477
Epoch: 60  Train Loss: 58771.755929028826  Validation Loss: 50336.978147866685
Epoch: 70  Train Loss: 47334.331284038344  Validation Loss: 44533.55943333804
Epoch: 80  Train Loss: 36204.90317638583  Validation Loss: 43115.85508737985
Epoch: 90  Train Loss: 26881.39890449024  Validation Loss: 44212.47958938523
Epoch: 100  Train Loss: 19148.038517999856  Validation Loss: 43352.353405077076
Epoch: 110  Train Loss: 11950.987879551698  Validation Loss: 36218.51131125034
Epoch: 120  Train Loss: 6103.65841024055  Validation Loss: 25859.198519563397
Epoch: 130  Train Loss: 11858.18121428795  Validation Loss: 25330.837347846806
Epoch: 140  Train Loss: 6038.218905941409  Validation Loss: 16941.454972200834
Epoch: 150  Train Loss: 2659.9236614668266  Validation Loss: 13808.612532000558
Epoch: 160  Train Loss: 4341.881653181418  Validation Loss: 18045.837737400692
Epoch: 170  Train Loss: 2924.857581017893  Validation Loss: 12815.484475909461
Epoch: 180  Train Loss: 1925.496566116962  Validation Loss: 13991.307239761923
Epoch: 190  Train Loss: 1828.9693052179173  Validation Loss: 11286.49451850597
Epoch: 200  Train Loss: 3503.653909831206  Validation Loss: 8930.325998534789
Epoch: 210  Train Loss: 1617.9351096226185  Validation Loss: 8467.985160253429
Epoch: 220  Train Loss: 1413.5907526112142  Validation Loss: 8190.803240964301
Epoch: 230  Train Loss: 1493.744126937397  Validation Loss: 9014.239304707626
Epoch: 240  Train Loss: 1762.4738841679343  Validation Loss: 10865.575623352956
Epoch: 250  Train Loss: 1436.0992263656146  Validation Loss: 7444.283496477528
Epoch: 260  Train Loss: 1263.1131892164424  Validation Loss: 7180.802278516637
Epoch: 270  Train Loss: 1393.0899122245376  Validation Loss: 10884.517029024688
Epoch: 280  Train Loss: 1246.0391876314588  Validation Loss: 8850.602827302979
Epoch: 290  Train Loss: 1074.175199014686  Validation Loss: 7231.669987615073
Epoch: 300  Train Loss: 1149.756870513593  Validation Loss: 7059.656774397945
Epoch: 310  Train Loss: 1139.5524180791579  Validation Loss: 6883.18544148367
Epoch: 320  Train Loss: 1206.2990640168773  Validation Loss: 6819.998597368254
Epoch: 330  Train Loss: 961.1146267050134  Validation Loss: 6783.936536090534
Epoch: 340  Train Loss: 1352.6827684325544  Validation Loss: 6217.844258374507
Epoch: 350  Train Loss: 929.143762601179  Validation Loss: 5138.096930135748
Epoch: 360  Train Loss: 778.3867665959766  Validation Loss: 4900.2884232650185
Epoch: 370  Train Loss: 1032.1664551616666  Validation Loss: 4899.487582928921
Epoch: 380  Train Loss: 1069.5722780384465  Validation Loss: 5989.172898664668
Epoch: 390  Train Loss: 740.7810625109865  Validation Loss: 3737.942974359935
Epoch: 400  Train Loss: 857.2773452248151  Validation Loss: 3260.354494106116
Epoch: 410  Train Loss: 958.3703936255315  Validation Loss: 4104.465255842515
Epoch: 420  Train Loss: 1111.124809562445  Validation Loss: 3818.33572996001
Epoch: 430  Train Loss: 788.2148351893762  Validation Loss: 3375.6700277531213
Epoch: 440  Train Loss: 889.7844435231151  Validation Loss: 2699.5208044282026
Epoch: 450  Train Loss: 760.3495456108964  Validation Loss: 3070.333328084081
Epoch: 460  Train Loss: 813.8693098095285  Validation Loss: 2319.1929031990526
Epoch: 470  Train Loss: 705.883192793007  Validation Loss: 2742.489234715464
Epoch: 480  Train Loss: 1076.472466382367  Validation Loss: 3143.848984782603
Epoch: 490  Train Loss: 905.9158592675576  Validation Loss: 2244.077373421462
Epoch: 500  Train Loss: 964.8522175865255  Validation Loss: 2455.391061904205
Epoch: 510  Train Loss: 803.7215680088212  Validation Loss: 2776.095996420153
Epoch: 520  Train Loss: 983.1662156100105  Validation Loss: 2390.969569036032
Epoch: 530  Train Loss: 780.8197053560525  Validation Loss: 1949.0302792244788
Epoch: 540  Train Loss: 974.7042980342776  Validation Loss: 3814.6059000375244
Epoch: 550  Train Loss: 805.4137372084498  Validation Loss: 1975.513684293436
Epoch: 560  Train Loss: 747.993856178481  Validation Loss: 2627.2287681569446
Epoch: 570  Train Loss: 599.900926265041  Validation Loss: 1765.0663748277766
Epoch: 580  Train Loss: 665.86602645754  Validation Loss: 2637.7356325513024
Epoch: 590  Train Loss: 567.1563912236172  Validation Loss: 2390.261779605074
Epoch: 600  Train Loss: 770.6767076422624  Validation Loss: 1564.8181473845511
Epoch: 610  Train Loss: 945.488943990859  Validation Loss: 3510.5328000361555
Epoch: 620  Train Loss: 928.1510676487322  Validation Loss: 2208.9570239832824
Epoch: 630  Train Loss: 582.1110378033057  Validation Loss: 1688.9808758304507
Epoch: 640  Train Loss: 588.9617208731312  Validation Loss: 2532.777062173852
Epoch: 650  Train Loss: 2272.1378768879863  Validation Loss: 2482.0935185609865
Epoch: 660  Train Loss: 659.1353818437116  Validation Loss: 2272.679505773752
Epoch: 670  Train Loss: 715.6364624194599  Validation Loss: 1586.517844657027
Epoch: 680  Train Loss: 651.0230571348326  Validation Loss: 1573.6157121463339
Epoch: 690  Train Loss: 662.4369201388686  Validation Loss: 2889.110028297085
Epoch: 700  Train Loss: 638.7178343566214  Validation Loss: 1710.9841428456139
Epoch: 710  Train Loss: 644.1582181022001  Validation Loss: 1401.9350324444483
Epoch: 720  Train Loss: 696.9174619814148  Validation Loss: 2301.7292839103616
Epoch: 730  Train Loss: 682.2307097377179  Validation Loss: 1876.4253350582958
Epoch: 740  Train Loss: 699.53197780029  Validation Loss: 1603.9662916674167
Epoch: 750  Train Loss: 696.8658552369349  Validation Loss: 3729.5422968291778
Epoch: 760  Train Loss: 554.1619388117463  Validation Loss: 1353.5640485504528
Epoch: 770  Train Loss: 475.7745761113549  Validation Loss: 1463.7843806504818
Epoch: 780  Train Loss: 668.2904988545409  Validation Loss: 2919.02511667351
Epoch: 790  Train Loss: 592.5064451688123  Validation Loss: 1373.1891430903424
Epoch: 800  Train Loss: 2402.8517355620593  Validation Loss: 1812.7543517073264
Epoch: 810  Train Loss: 1190.6214302588803  Validation Loss: 4836.045551947276
Epoch: 820  Train Loss: 764.2860308842284  Validation Loss: 1775.3248849279385
Epoch: 830  Train Loss: 632.2711738583035  Validation Loss: 1734.2159747510323
Epoch: 840  Train Loss: 1088.911457900191  Validation Loss: 1579.271928738621
Epoch: 850  Train Loss: 602.8920438569927  Validation Loss: 1806.713239777114
Epoch: 860  Train Loss: 403.11859636810397  Validation Loss: 1440.4392693559794
Epoch: 870  Train Loss: 541.4372698261802  Validation Loss: 1437.4232003162708
Epoch: 880  Train Loss: 378.27814810836026  Validation Loss: 1400.707557002696
Epoch: 890  Train Loss: 398.95592573783125  Validation Loss: 1596.2702318151537
Epoch: 900  Train Loss: 620.2894394555657  Validation Loss: 1308.5244213440387
Epoch: 910  Train Loss: 571.6748281597243  Validation Loss: 1345.2171464136295
Epoch: 920  Train Loss: 515.9254062874039  Validation Loss: 1640.6067939056074
Epoch: 930  Train Loss: 892.9778193549973  Validation Loss: 1607.7000658430284
Epoch: 940  Train Loss: 631.9003138142095  Validation Loss: 1480.5753169732366
Epoch: 950  Train Loss: 477.83189965291905  Validation Loss: 1214.2137774381824
Epoch: 960  Train Loss: 498.41711517781164  Validation Loss: 4225.273803288317
Epoch: 970  Train Loss: 610.4936371985156  Validation Loss: 1371.8607208021094
Epoch: 980  Train Loss: 361.68861398720463  Validation Loss: 1637.5439304686402
Epoch: 990  Train Loss: 580.3852739904804  Validation Loss: 1897.3230729867964
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(1267.3269, device='cuda:0', grad_fn=<DivBackward0>)