Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83767.08123377773  Validation Loss: 81551.74383123925
Epoch: 20  Train Loss: 82432.66265970503  Validation Loss: 78092.12364685509
Epoch: 30  Train Loss: 79966.20109553768  Validation Loss: 73853.16643320455
Epoch: 40  Train Loss: 75801.8408642998  Validation Loss: 66840.41841857346
Epoch: 50  Train Loss: 69835.71593749133  Validation Loss: 57558.86574930263
Epoch: 60  Train Loss: 61603.00711231012  Validation Loss: 49771.84785945242
Epoch: 70  Train Loss: 52332.01945687856  Validation Loss: 44582.61167460564
Epoch: 80  Train Loss: 42526.14327388653  Validation Loss: 44432.33169852844
Epoch: 90  Train Loss: 34837.17456028959  Validation Loss: 47063.935362429824
Epoch: 100  Train Loss: 28387.30905173713  Validation Loss: 48518.764299583214
Epoch: 110  Train Loss: 23250.703425717675  Validation Loss: 46078.43625891416
Epoch: 120  Train Loss: 16534.046254449666  Validation Loss: 39946.37402433774
Epoch: 130  Train Loss: 9861.64438031793  Validation Loss: 32354.528265316545
Epoch: 140  Train Loss: 6380.6865359053  Validation Loss: 23114.476281054758
Epoch: 150  Train Loss: 6252.236974178246  Validation Loss: 15496.153864068094
Epoch: 160  Train Loss: 5048.412031896697  Validation Loss: 14088.892441185062
Epoch: 170  Train Loss: 6415.64412985427  Validation Loss: 11127.981197173249
Epoch: 180  Train Loss: 4609.373064339765  Validation Loss: 8124.925770164371
Epoch: 190  Train Loss: 4803.0345163014335  Validation Loss: 8355.4621450556
Epoch: 200  Train Loss: 4342.687657988142  Validation Loss: 7894.282839976942
Epoch: 210  Train Loss: 4670.333106604888  Validation Loss: 7704.0505036825525
Epoch: 220  Train Loss: 3933.3862154013764  Validation Loss: 7103.542543170852
Epoch: 230  Train Loss: 4360.846787579823  Validation Loss: 7503.534989684664
Epoch: 240  Train Loss: 3937.5116546080376  Validation Loss: 9170.28207497352
Epoch: 250  Train Loss: 3570.426451662364  Validation Loss: 8610.630047532642
Epoch: 260  Train Loss: 3399.5703747885145  Validation Loss: 6328.202422561639
Epoch: 270  Train Loss: 3508.2120884293813  Validation Loss: 6282.743730985108
Epoch: 280  Train Loss: 3348.9309867878183  Validation Loss: 6224.252488245885
Epoch: 290  Train Loss: 3502.395295778475  Validation Loss: 6062.486634515582
Epoch: 300  Train Loss: 3561.7837808242457  Validation Loss: 5614.140390181287
Epoch: 310  Train Loss: 3510.790715329893  Validation Loss: 6498.578209672436
Epoch: 320  Train Loss: 3192.4326290595586  Validation Loss: 5337.261747457809
Epoch: 330  Train Loss: 2136.3839850304994  Validation Loss: 3471.0731364238877
Epoch: 340  Train Loss: 1757.9193985349386  Validation Loss: 3925.059255557245
Epoch: 350  Train Loss: 1618.4063371126715  Validation Loss: 1988.7208936179845
Epoch: 360  Train Loss: 1520.0086197318024  Validation Loss: 2667.671664597851
Epoch: 370  Train Loss: 1583.3182615521882  Validation Loss: 3804.153344656944
Epoch: 380  Train Loss: 1567.969406266883  Validation Loss: 2404.7326500437684
Epoch: 390  Train Loss: 1426.5422064192164  Validation Loss: 2545.4499865711095
Epoch: 400  Train Loss: 1298.1349785562563  Validation Loss: 2284.17290470938
Epoch: 410  Train Loss: 1182.708782813651  Validation Loss: 1897.1982901449871
Epoch: 420  Train Loss: 1591.5640544324826  Validation Loss: 2495.5547805166216
Epoch: 430  Train Loss: 1131.8150206182972  Validation Loss: 1868.4270842458504
Epoch: 440  Train Loss: 1224.907355330765  Validation Loss: 3308.664115087983
Epoch: 450  Train Loss: 1020.5422768800527  Validation Loss: 2198.3187973076465
Epoch: 460  Train Loss: 1083.260296475375  Validation Loss: 1738.7477833630555
Epoch: 470  Train Loss: 1202.4453130405798  Validation Loss: 3678.5318195256077
Epoch: 480  Train Loss: 877.185572574811  Validation Loss: 1639.2124056334296
Epoch: 490  Train Loss: 1485.219402497455  Validation Loss: 4498.524990664233
Epoch: 500  Train Loss: 870.8057895423676  Validation Loss: 1773.3599428867944
Epoch: 510  Train Loss: 847.1102160356851  Validation Loss: 1865.8564166734
Epoch: 520  Train Loss: 1267.842616250028  Validation Loss: 1973.524314942453
Epoch: 530  Train Loss: 816.9770424935518  Validation Loss: 2158.122985580139
Epoch: 540  Train Loss: 804.471401522096  Validation Loss: 2515.7690261360085
Epoch: 550  Train Loss: 843.0977128582598  Validation Loss: 1560.343855406631
Epoch: 560  Train Loss: 1165.401919460541  Validation Loss: 1620.159104824746
Epoch: 570  Train Loss: 747.7799311544019  Validation Loss: 2046.4774586685462
Epoch: 580  Train Loss: 699.1423222609204  Validation Loss: 1576.161607543829
Epoch: 590  Train Loss: 1149.5595923849244  Validation Loss: 1837.397653884971
Epoch: 600  Train Loss: 865.1852039592039  Validation Loss: 1885.0415437729994
Epoch: 610  Train Loss: 681.8828646512734  Validation Loss: 2079.6702852491217
Epoch: 620  Train Loss: 722.9126449744026  Validation Loss: 1686.2816263896405
Epoch: 630  Train Loss: 1653.2758168012144  Validation Loss: 7462.17623577867
Epoch: 640  Train Loss: 1352.6581818303946  Validation Loss: 1951.5008019772156
Epoch: 650  Train Loss: 880.6400995694318  Validation Loss: 1639.8818434743341
Epoch: 660  Train Loss: 977.4434107053818  Validation Loss: 1859.848007117021
Epoch: 670  Train Loss: 733.2750968677868  Validation Loss: 1561.7710258312725
Epoch: 680  Train Loss: 756.6881832600766  Validation Loss: 1485.2203020661618
Epoch: 690  Train Loss: 1142.0116931522962  Validation Loss: 2987.3828362787103
Epoch: 700  Train Loss: 681.214529081753  Validation Loss: 1616.6589472446278
Epoch: 710  Train Loss: 804.6597343472395  Validation Loss: 1663.429253065366
Epoch: 720  Train Loss: 669.1874312202439  Validation Loss: 1963.3078164363696
Epoch: 730  Train Loss: 655.6543962694763  Validation Loss: 1819.4409664995017
Epoch: 740  Train Loss: 676.219644248963  Validation Loss: 1657.8629380911614
Epoch: 750  Train Loss: 685.1659819619144  Validation Loss: 3147.505404554634
Epoch: 760  Train Loss: 660.4493757057188  Validation Loss: 1555.3955917162803
Epoch: 770  Train Loss: 666.9881846163358  Validation Loss: 2239.8672566216705
Epoch: 780  Train Loss: 703.73803481124  Validation Loss: 1773.557491374567
Epoch: 790  Train Loss: 687.1550064101684  Validation Loss: 2372.164951933005
Epoch: 800  Train Loss: 671.8114790510493  Validation Loss: 2356.5889018257662
Epoch: 810  Train Loss: 776.0963831567072  Validation Loss: 1446.1253089263687
Epoch: 820  Train Loss: 812.1586510698445  Validation Loss: 2383.6867149483855
Epoch: 830  Train Loss: 646.4637756524056  Validation Loss: 1540.7279420041182
Epoch: 840  Train Loss: 913.6104137396818  Validation Loss: 1994.3420348749332
Epoch: 850  Train Loss: 643.2272144847337  Validation Loss: 1902.0342412551902
Epoch: 860  Train Loss: 524.9225078601586  Validation Loss: 1533.808584585743
Epoch: 870  Train Loss: 698.6072768950438  Validation Loss: 2054.7329842412255
Epoch: 880  Train Loss: 656.9889779812687  Validation Loss: 1667.7468346022395
Epoch: 890  Train Loss: 565.7806461769301  Validation Loss: 1648.9102595308939
Epoch: 900  Train Loss: 617.661959426974  Validation Loss: 1356.092434694843
Epoch: 910  Train Loss: 674.4083721195707  Validation Loss: 2161.5239716884953
Epoch: 920  Train Loss: 562.4529786338621  Validation Loss: 1924.5522431668594
Epoch: 930  Train Loss: 489.7948686494362  Validation Loss: 1361.5663567443441
Epoch: 940  Train Loss: 800.5383969600808  Validation Loss: 4526.852423332254
Epoch: 950  Train Loss: 969.3333457525941  Validation Loss: 5649.3926811852225
Epoch: 960  Train Loss: 1337.2359158719637  Validation Loss: 8921.70732889587
Epoch: 970  Train Loss: 1400.9546768472555  Validation Loss: 2884.9336872438867
Epoch: 980  Train Loss: 1318.7122298996856  Validation Loss: 2260.2723495728037
Epoch: 990  Train Loss: 1050.8514050162378  Validation Loss: 2218.01093258446
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(2062.2156, device='cuda:0', grad_fn=<DivBackward0>)