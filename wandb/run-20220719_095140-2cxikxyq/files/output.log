Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 15305.948038975783  Validation Loss: 15331.094673931902
Epoch: 20  Train Loss: 15096.636410614301  Validation Loss: 15125.287448617622
Epoch: 30  Train Loss: 15102.255733926011  Validation Loss: 15170.25224503663
Epoch: 40  Train Loss: 15227.689441063463  Validation Loss: 15178.478403759882
Epoch: 50  Train Loss: 15145.354223289643  Validation Loss: 15033.586838439185
Epoch: 60  Train Loss: 15198.793293951312  Validation Loss: 15184.640204817853
Epoch: 70  Train Loss: 15062.557907672175  Validation Loss: 15182.449144906675
Epoch: 80  Train Loss: 15042.826230912833  Validation Loss: 15115.024094147288
Epoch: 90  Train Loss: 15160.105645010803  Validation Loss: 15083.437643800677
Epoch: 100  Train Loss: 15329.670617167772  Validation Loss: 14754.44032736721
Epoch: 110  Train Loss: 15248.746750450102  Validation Loss: 15068.351128956983
Epoch: 120  Train Loss: 15511.164721766292  Validation Loss: 15121.52268644904
Epoch: 130  Train Loss: 15169.126394784114  Validation Loss: 15069.90671911488
Epoch: 140  Train Loss: 15164.27773016609  Validation Loss: 15264.168628735386
Epoch: 150  Train Loss: 15201.449373081045  Validation Loss: 15253.807246565117
Epoch: 160  Train Loss: 15130.356016918033  Validation Loss: 15012.691813144638
Epoch: 170  Train Loss: 15210.852484418792  Validation Loss: 15055.323403065308
Epoch: 180  Train Loss: 15044.745081349984  Validation Loss: 15459.503718382293
Epoch: 190  Train Loss: 15088.570143263729  Validation Loss: 14923.432264645493
Epoch: 200  Train Loss: 15404.819701165552  Validation Loss: 14995.515907969213
Epoch: 210  Train Loss: 15218.273132855838  Validation Loss: 14991.549551680017
Epoch: 220  Train Loss: 15365.665195990072  Validation Loss: 14786.481522096052
Epoch: 230  Train Loss: 15184.880233806582  Validation Loss: 15504.56543652647
Epoch: 240  Train Loss: 15143.539860343235  Validation Loss: 15146.006797193777
Epoch: 250  Train Loss: 15235.322219001988  Validation Loss: 15038.527016452419
Epoch: 260  Train Loss: 15258.671932846664  Validation Loss: 15524.639952840518
Epoch: 270  Train Loss: 15302.58020484419  Validation Loss: 15074.292383982804
Epoch: 280  Train Loss: 15137.097661871716  Validation Loss: 15109.483957706916
Epoch: 290  Train Loss: 15192.040183741117  Validation Loss: 14735.87012134835
Epoch: 300  Train Loss: 14877.821722535917  Validation Loss: 14854.968140606714
Epoch: 310  Train Loss: 15215.792412282657  Validation Loss: 15218.647203811908
Epoch: 320  Train Loss: 15186.47421422589  Validation Loss: 15131.191627432392
Epoch: 330  Train Loss: 15101.864548531912  Validation Loss: 15197.719759311425
Epoch: 340  Train Loss: 15223.150361488018  Validation Loss: 14950.176140843421
Epoch: 350  Train Loss: 15226.74070706824  Validation Loss: 15170.03946758077
Epoch: 360  Train Loss: 15126.647426988522  Validation Loss: 15327.974354824968
Epoch: 370  Train Loss: 15398.053870460752  Validation Loss: 14836.008384072593
Epoch: 380  Train Loss: 15261.293619306125  Validation Loss: 15233.814918223236
Epoch: 390  Train Loss: 15107.87405692111  Validation Loss: 15223.62814215245
Epoch: 400  Train Loss: 15149.411426538112  Validation Loss: 15047.15582755888
Epoch: 410  Train Loss: 15307.016197012525  Validation Loss: 15241.097402771014
Epoch: 420  Train Loss: 15276.117962984028  Validation Loss: 15288.471316849042
Epoch: 430  Train Loss: 15136.494771821075  Validation Loss: 15000.576726745205
Epoch: 440  Train Loss: 15238.76496447545  Validation Loss: 15053.993342178248
Epoch: 450  Train Loss: 15360.488500706686  Validation Loss: 15476.028770899662
Epoch: 460  Train Loss: 15252.97627327791  Validation Loss: 15242.5199264984
Epoch: 470  Train Loss: 15139.803317754151  Validation Loss: 15027.263446016286
Epoch: 480  Train Loss: 15138.952353311366  Validation Loss: 15024.706380220696
Epoch: 490  Train Loss: 15247.786690760986  Validation Loss: 15105.749883468065
Epoch: 500  Train Loss: 15261.935830526907  Validation Loss: 15137.154797078309
Epoch: 510  Train Loss: 15007.20644239753  Validation Loss: 15238.447752348684
Epoch: 520  Train Loss: 15359.324376337767  Validation Loss: 15057.376842302272
Epoch: 530  Train Loss: 15316.337453330952  Validation Loss: 15012.160480841774
Epoch: 540  Train Loss: 15224.308381623434  Validation Loss: 15002.199000011655
Epoch: 550  Train Loss: 15287.456307227127  Validation Loss: 15042.232565649085
Epoch: 560  Train Loss: 15181.760844405531  Validation Loss: 15447.148786435433
Epoch: 570  Train Loss: 15146.811551206405  Validation Loss: 15374.52406847765
Epoch: 580  Train Loss: 15362.12866302454  Validation Loss: 15015.148420583546
Epoch: 590  Train Loss: 15232.957986214487  Validation Loss: 15353.238306906602
Epoch: 600  Train Loss: 15251.33016284392  Validation Loss: 14944.636121717946
Epoch: 610  Train Loss: 15105.468392221177  Validation Loss: 15241.590137518482
Epoch: 620  Train Loss: 15247.956634937995  Validation Loss: 15280.639629699126
Epoch: 630  Train Loss: 15137.975812938885  Validation Loss: 15336.18989913439
Epoch: 640  Train Loss: 15099.115641349907  Validation Loss: 14831.35593137346
Epoch: 650  Train Loss: 15338.788807624062  Validation Loss: 15391.193524295339
Epoch: 660  Train Loss: 15294.00296122402  Validation Loss: 15011.691354690833
Epoch: 670  Train Loss: 15080.647578286553  Validation Loss: 15023.696788814033
Epoch: 680  Train Loss: 15196.668176466506  Validation Loss: 15073.743702927204
Epoch: 690  Train Loss: 15044.14430671765  Validation Loss: 15282.456274823457
Epoch: 700  Train Loss: 15326.400393018957  Validation Loss: 15132.45094234382
Epoch: 710  Train Loss: 15384.585641591053  Validation Loss: 15130.304045820707
Epoch: 720  Train Loss: 15069.29175142057  Validation Loss: 14830.499135111677
Epoch: 730  Train Loss: 15264.15120405794  Validation Loss: 14984.133836586634
Epoch: 740  Train Loss: 15321.018350447559  Validation Loss: 15096.732596178726
Epoch: 750  Train Loss: 15341.027932328134  Validation Loss: 14910.08547434784
Epoch: 760  Train Loss: 15169.74560232616  Validation Loss: 15081.656877697578
Epoch: 770  Train Loss: 15282.861581208688  Validation Loss: 15133.426713475734
Epoch: 780  Train Loss: 15254.031910773438  Validation Loss: 15381.68968755324
Epoch: 790  Train Loss: 15235.592748333356  Validation Loss: 15484.311855749962
Epoch: 800  Train Loss: 15230.121648330687  Validation Loss: 14918.810333809526
Epoch: 810  Train Loss: 15032.743001860625  Validation Loss: 15283.096508196353
Epoch: 820  Train Loss: 15323.940831011036  Validation Loss: 14928.04185995579
Epoch: 830  Train Loss: 15242.72294880345  Validation Loss: 15041.161947576353
Epoch: 840  Train Loss: 15195.36102612893  Validation Loss: 15440.639555881613
Epoch: 850  Train Loss: 15096.74219071107  Validation Loss: 15167.229543486617
Epoch: 860  Train Loss: 15298.478101564417  Validation Loss: 15043.143560146487
Epoch: 870  Train Loss: 15352.246115288164  Validation Loss: 15371.341097859535
Epoch: 880  Train Loss: 15152.811342750198  Validation Loss: 15043.254522108922
Epoch: 890  Train Loss: 15086.543759868591  Validation Loss: 14884.406511377427
Epoch: 900  Train Loss: 14988.049307754494  Validation Loss: 15233.293932098044
Epoch: 910  Train Loss: 15405.147516372452  Validation Loss: 15477.843369520415
Epoch: 920  Train Loss: 15236.569222668013  Validation Loss: 15225.84185598381
Epoch: 930  Train Loss: 15262.578238448832  Validation Loss: 15058.675426492253
Epoch: 940  Train Loss: 15066.544795320702  Validation Loss: 15113.410056508877
Epoch: 950  Train Loss: 15159.039282454027  Validation Loss: 14953.984790070723
Epoch: 960  Train Loss: 15103.687339582444  Validation Loss: 15111.100141379693
Epoch: 970  Train Loss: 15162.064742565728  Validation Loss: 15406.439561635594
Epoch: 980  Train Loss: 15087.76049143926  Validation Loss: 15131.770064654607
Epoch: 990  Train Loss: 15117.423360189832  Validation Loss: 15061.345828949408
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(79214.2031, device='cuda:0', grad_fn=<DivBackward0>)