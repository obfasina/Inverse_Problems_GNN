Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83600.1860163569  Validation Loss: 81775.90123217975
Epoch: 20  Train Loss: 82265.57309676816  Validation Loss: 78205.51081754378
Epoch: 30  Train Loss: 79299.38227483908  Validation Loss: 74714.45418499417
Epoch: 40  Train Loss: 74190.56566048601  Validation Loss: 66318.91184657342
Epoch: 50  Train Loss: 66372.81485238457  Validation Loss: 56501.93304396665
Epoch: 60  Train Loss: 56407.29564215619  Validation Loss: 47983.830798118965
Epoch: 70  Train Loss: 44065.67119198018  Validation Loss: 44217.23435843484
Epoch: 80  Train Loss: 32073.730494212712  Validation Loss: 45136.09397850501
Epoch: 90  Train Loss: 21360.082769490178  Validation Loss: 46844.003119530586
Epoch: 100  Train Loss: 14009.455832066393  Validation Loss: 46042.23012970511
Epoch: 110  Train Loss: 7141.095829825994  Validation Loss: 41971.65671898183
Epoch: 120  Train Loss: 3944.5214610816765  Validation Loss: 36894.14206247431
Epoch: 130  Train Loss: 2936.681683186743  Validation Loss: 26513.635426650075
Epoch: 140  Train Loss: 2408.921026670496  Validation Loss: 32049.409342140785
Epoch: 150  Train Loss: 2255.6418981951565  Validation Loss: 24241.899292917846
Epoch: 160  Train Loss: 1877.134910376771  Validation Loss: 25122.7502387136
Epoch: 170  Train Loss: 2589.6129561284833  Validation Loss: 36523.681922763644
Epoch: 180  Train Loss: 1657.15247025453  Validation Loss: 20153.87843603129
Epoch: 190  Train Loss: 1512.1511860321723  Validation Loss: 18626.534065477605
Epoch: 200  Train Loss: 2179.0363611760267  Validation Loss: 20737.99262051676
Epoch: 210  Train Loss: 1712.063438806652  Validation Loss: 20435.63031329711
Epoch: 220  Train Loss: 1632.5683558649623  Validation Loss: 23266.502594419137
Epoch: 230  Train Loss: 1592.7204063165202  Validation Loss: 35072.41101292646
Epoch: 240  Train Loss: 1498.1063539733752  Validation Loss: 16933.076421039594
Epoch: 250  Train Loss: 1714.558114073751  Validation Loss: 21314.62623115892
Epoch: 260  Train Loss: 1377.8900724631596  Validation Loss: 15999.121333707799
Epoch: 270  Train Loss: 1222.8887734091256  Validation Loss: 16406.778728976355
Epoch: 280  Train Loss: 1439.1362542267125  Validation Loss: 14623.987949411046
Epoch: 290  Train Loss: 1247.7760562769606  Validation Loss: 17648.7340536359
Epoch: 300  Train Loss: 1375.097668939198  Validation Loss: 14238.684981967079
Epoch: 310  Train Loss: 1351.7450706182638  Validation Loss: 13167.517617047231
Epoch: 320  Train Loss: 1187.3078757719018  Validation Loss: 13380.532850897018
Epoch: 330  Train Loss: 1731.945136273668  Validation Loss: 13018.882722940243
Epoch: 340  Train Loss: 1188.3209448130865  Validation Loss: 12277.187868992225
Epoch: 350  Train Loss: 1133.279631751746  Validation Loss: 12017.227565257173
Epoch: 360  Train Loss: 1193.412302897976  Validation Loss: 14067.289583023872
Epoch: 370  Train Loss: 1046.5314206804703  Validation Loss: 12238.80286423917
Epoch: 380  Train Loss: 1403.4397549206526  Validation Loss: 11746.892321866526
Epoch: 390  Train Loss: 991.3692135298278  Validation Loss: 13036.100263390372
Epoch: 400  Train Loss: 1147.4467280145145  Validation Loss: 11595.921282987036
Epoch: 410  Train Loss: 1016.4147210143816  Validation Loss: 11617.973264466485
Epoch: 420  Train Loss: 941.9656408664814  Validation Loss: 11490.123209442214
Epoch: 430  Train Loss: 1136.5352067179608  Validation Loss: 13249.998693484877
Epoch: 440  Train Loss: 1020.3535697481339  Validation Loss: 11848.33349723609
Epoch: 450  Train Loss: 1074.1349138533171  Validation Loss: 12753.700893454927
Epoch: 460  Train Loss: 957.3875302879911  Validation Loss: 11518.415321721255
Epoch: 470  Train Loss: 1009.892327815801  Validation Loss: 12144.710368637172
Epoch: 480  Train Loss: 1415.1798270131446  Validation Loss: 11319.693930610316
Epoch: 490  Train Loss: 1193.2769595284815  Validation Loss: 11404.87312337427
Epoch: 500  Train Loss: 1049.4722900715972  Validation Loss: 11112.648518478985
Epoch: 510  Train Loss: 868.0330359873982  Validation Loss: 11273.305807447916
Epoch: 520  Train Loss: 830.4106466922218  Validation Loss: 12207.659478791002
Epoch: 530  Train Loss: 987.5609577963374  Validation Loss: 11085.429519664605
Epoch: 540  Train Loss: 878.3956759062812  Validation Loss: 11247.589204297627
Epoch: 550  Train Loss: 920.8186184311304  Validation Loss: 11384.737982270788
Epoch: 560  Train Loss: 864.6539261650709  Validation Loss: 12560.553548974645
Epoch: 570  Train Loss: 1248.3605108911715  Validation Loss: 11994.507757972138
Epoch: 580  Train Loss: 901.4664179608889  Validation Loss: 11424.372448481101
Epoch: 590  Train Loss: 1332.6403078089977  Validation Loss: 10841.947916197852
Epoch: 600  Train Loss: 770.8130530034932  Validation Loss: 11300.243249872125
Epoch: 610  Train Loss: 808.8426346709397  Validation Loss: 11139.676142348386
Epoch: 620  Train Loss: 944.058892953912  Validation Loss: 10947.448510805392
Epoch: 630  Train Loss: 858.9405101413214  Validation Loss: 11000.135428629854
Epoch: 640  Train Loss: 779.5650334469811  Validation Loss: 11344.397081797748
Epoch: 650  Train Loss: 947.6064075891921  Validation Loss: 10795.719806652385
Epoch: 660  Train Loss: 770.6498423745701  Validation Loss: 10771.245829231131
Epoch: 670  Train Loss: 738.5717879966002  Validation Loss: 10522.519233425426
Epoch: 680  Train Loss: 970.116136661095  Validation Loss: 11207.325428665712
Epoch: 690  Train Loss: 1251.147864274851  Validation Loss: 10201.355361288845
Epoch: 700  Train Loss: 772.3297887110971  Validation Loss: 10507.22971604651
Epoch: 710  Train Loss: 930.7403435542469  Validation Loss: 10625.961797523438
Epoch: 720  Train Loss: 987.778583803823  Validation Loss: 10134.320208975814
Epoch: 730  Train Loss: 909.0129862385369  Validation Loss: 10188.90190689023
Epoch: 740  Train Loss: 764.2882639073454  Validation Loss: 10079.314592787796
Epoch: 750  Train Loss: 698.5099804751754  Validation Loss: 10022.968798124013
Epoch: 760  Train Loss: 851.66153984311  Validation Loss: 10309.865979935546
Epoch: 770  Train Loss: 723.0782329014037  Validation Loss: 9995.51246914219
Epoch: 780  Train Loss: 707.5026619539203  Validation Loss: 9826.415503192033
Epoch: 790  Train Loss: 590.2572646076418  Validation Loss: 9992.10377354942
Epoch: 800  Train Loss: 639.1780104839853  Validation Loss: 10691.074303951647
Epoch: 810  Train Loss: 570.8584934424442  Validation Loss: 9717.854271270928
Epoch: 820  Train Loss: 900.7536222940894  Validation Loss: 9924.850224017277
Epoch: 830  Train Loss: 727.0822279011098  Validation Loss: 10517.873906749912
Epoch: 840  Train Loss: 747.7815483050734  Validation Loss: 9997.731839131968
Epoch: 850  Train Loss: 552.0202814168427  Validation Loss: 9599.696946574215
Epoch: 860  Train Loss: 747.4790781579613  Validation Loss: 10949.438513980453
Epoch: 870  Train Loss: 925.3205663409384  Validation Loss: 9630.753246136617
Epoch: 880  Train Loss: 1253.1995887970327  Validation Loss: 9893.968260216703
Epoch: 890  Train Loss: 663.9396473316191  Validation Loss: 9369.227265361365
Epoch: 900  Train Loss: 663.0521806455761  Validation Loss: 9139.069484647973
Epoch: 910  Train Loss: 541.098739546608  Validation Loss: 8863.309865288998
Epoch: 920  Train Loss: 782.3979918774136  Validation Loss: 8854.213993727442
Epoch: 930  Train Loss: 717.0974695384388  Validation Loss: 11892.924951265115
Epoch: 940  Train Loss: 568.0359396268509  Validation Loss: 8343.869403366998
Epoch: 950  Train Loss: 628.6179002460783  Validation Loss: 9430.547967056944
Epoch: 960  Train Loss: 680.5148065304585  Validation Loss: 9596.052413961645
Epoch: 970  Train Loss: 636.9926277009955  Validation Loss: 10030.058321963079
Epoch: 980  Train Loss: 675.2958719766028  Validation Loss: 9566.91214276348
Epoch: 990  Train Loss: 642.4669169620666  Validation Loss: 9453.432083180209
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(9352.4492, device='cuda:0', grad_fn=<DivBackward0>)