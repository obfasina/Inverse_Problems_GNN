Time to play:
Loading lists of data
graph data Data(x=[81, 1], edge_index=[2, 324], y=[81], pos=[81, 2])
Epoch: 10  Train Loss: 83805.62863997935  Validation Loss: 82071.24526980094
Epoch: 20  Train Loss: 82868.58578647616  Validation Loss: 79475.10488399255
Epoch: 30  Train Loss: 80897.17196402377  Validation Loss: 76549.01368007722
Epoch: 40  Train Loss: 77126.85245790928  Validation Loss: 68808.0739297524
Epoch: 50  Train Loss: 71618.70805886296  Validation Loss: 59646.81767236795
Epoch: 60  Train Loss: 64520.41580999988  Validation Loss: 51754.09482927618
Epoch: 70  Train Loss: 55722.094772493154  Validation Loss: 45744.55388086148
Epoch: 80  Train Loss: 46773.91744364129  Validation Loss: 43864.67773513747
Epoch: 90  Train Loss: 38755.91084704064  Validation Loss: 45512.8332226677
Epoch: 100  Train Loss: 32225.82624994036  Validation Loss: 47508.248004957924
Epoch: 110  Train Loss: 27395.489834835756  Validation Loss: 47793.74420552912
Epoch: 120  Train Loss: 22633.698754982208  Validation Loss: 43829.765887010144
Epoch: 130  Train Loss: 18135.965067754438  Validation Loss: 32672.91697155597
Epoch: 140  Train Loss: 14589.721462498143  Validation Loss: 28608.070161197105
Epoch: 150  Train Loss: 12171.643713447984  Validation Loss: 20044.604260285978
Epoch: 160  Train Loss: 11085.138028954767  Validation Loss: 16247.998757057483
Epoch: 170  Train Loss: 10908.378407270802  Validation Loss: 17855.34321728214
Epoch: 180  Train Loss: 10729.251028663295  Validation Loss: 14420.973560201333
Epoch: 190  Train Loss: 10644.054408569298  Validation Loss: 17857.768999344582
Epoch: 200  Train Loss: 10539.158972519399  Validation Loss: 16129.414251258257
Epoch: 210  Train Loss: 9112.730534436594  Validation Loss: 13903.405765529857
Epoch: 220  Train Loss: 3917.9087295108384  Validation Loss: 14348.243449459902
Epoch: 230  Train Loss: 3689.3622009607116  Validation Loss: 7541.826105058945
Epoch: 240  Train Loss: 8939.968211556681  Validation Loss: 11888.783127137393
Epoch: 250  Train Loss: 5453.327066647697  Validation Loss: 9424.381687565568
Epoch: 260  Train Loss: 5044.538841816221  Validation Loss: 8876.15440525822
Epoch: 270  Train Loss: 3501.6706220210795  Validation Loss: 4871.851551438266
Epoch: 280  Train Loss: 2810.3210276072186  Validation Loss: 10626.274758653724
Epoch: 290  Train Loss: 6660.308554936695  Validation Loss: 48554.3869713655
Epoch: 300  Train Loss: 5362.620371434998  Validation Loss: 16063.253098699644
Epoch: 310  Train Loss: 3570.6706325633354  Validation Loss: 8570.113343344074
Epoch: 320  Train Loss: 4024.623830751512  Validation Loss: 7463.017156532541
Epoch: 330  Train Loss: 2564.448333292356  Validation Loss: 7868.006798665084
Epoch: 340  Train Loss: 1579.346192314967  Validation Loss: 5568.485052982875
Epoch: 350  Train Loss: 1347.655679836706  Validation Loss: 5001.903985763459
Epoch: 360  Train Loss: 1402.2166347269547  Validation Loss: 6340.970444114036
Epoch: 370  Train Loss: 2138.767507517717  Validation Loss: 4703.287741256247
Epoch: 380  Train Loss: 2168.2146477600427  Validation Loss: 5552.252178205894
Epoch: 390  Train Loss: 2497.088162438196  Validation Loss: 4933.2493136281555
Epoch: 400  Train Loss: 1378.792200968822  Validation Loss: 4685.7351177203855
Epoch: 410  Train Loss: 1015.0353558323263  Validation Loss: 4774.628678256346
Epoch: 420  Train Loss: 1035.1760290658365  Validation Loss: 3883.596014351291
Epoch: 430  Train Loss: 968.8571139825976  Validation Loss: 3573.3185495279704
Epoch: 440  Train Loss: 1456.4044580665893  Validation Loss: 6547.967771956773
Epoch: 450  Train Loss: 2678.8491034636786  Validation Loss: 5125.465022206006
Epoch: 460  Train Loss: 1054.4042188940828  Validation Loss: 4045.5962038630605
Epoch: 470  Train Loss: 1083.6008582684135  Validation Loss: 3439.5092980496997
Epoch: 480  Train Loss: 834.4171297943327  Validation Loss: 3032.913440746508
Epoch: 490  Train Loss: 736.38857752345  Validation Loss: 3041.410281897591
Epoch: 500  Train Loss: 1044.0181100571556  Validation Loss: 3950.2573138968914
Epoch: 510  Train Loss: 1147.6877474380174  Validation Loss: 3359.2624710340374
Epoch: 520  Train Loss: 930.061278813631  Validation Loss: 3467.590264628427
Epoch: 530  Train Loss: 1114.291654902892  Validation Loss: 3393.2693079347823
Epoch: 540  Train Loss: 1067.8637519408987  Validation Loss: 3865.513428983029
Epoch: 550  Train Loss: 1307.2145584654259  Validation Loss: 4743.869675217413
Epoch: 560  Train Loss: 1062.7742739592616  Validation Loss: 3732.8557770932634
Epoch: 570  Train Loss: 1053.7537873759838  Validation Loss: 3972.8414314050506
Epoch: 580  Train Loss: 1024.2437601720806  Validation Loss: 3025.732776555165
Epoch: 590  Train Loss: 712.8328574454167  Validation Loss: 2766.408780229984
Epoch: 600  Train Loss: 816.1824906022294  Validation Loss: 3229.7741776758558
Epoch: 610  Train Loss: 1174.4095008826523  Validation Loss: 3161.193281788777
Epoch: 620  Train Loss: 995.0524709558722  Validation Loss: 2842.8383064845875
Epoch: 630  Train Loss: 676.5106915751415  Validation Loss: 3190.2323200848973
Epoch: 640  Train Loss: 843.032079295529  Validation Loss: 3929.367350260044
Epoch: 650  Train Loss: 933.749767956055  Validation Loss: 4563.461745783831
Epoch: 660  Train Loss: 1093.5043685675628  Validation Loss: 3756.8869856568895
Epoch: 670  Train Loss: 677.5449268664221  Validation Loss: 2694.467814174866
Epoch: 680  Train Loss: 1360.335528574247  Validation Loss: 4907.99872427038
Epoch: 690  Train Loss: 1294.6774228834558  Validation Loss: 2975.7606079398597
Epoch: 700  Train Loss: 904.3647123307077  Validation Loss: 2671.054485039875
Epoch: 710  Train Loss: 1088.6989495376338  Validation Loss: 2711.372538619129
Epoch: 720  Train Loss: 1527.2111355683135  Validation Loss: 2750.273953946923
Epoch: 730  Train Loss: 1215.6395397642552  Validation Loss: 2671.0046754183822
Epoch: 740  Train Loss: 1796.0399649673436  Validation Loss: 3605.89933417955
Epoch: 750  Train Loss: 929.1535441663806  Validation Loss: 2697.108750015313
Epoch: 760  Train Loss: 880.8153289989717  Validation Loss: 2634.877792094012
Epoch: 770  Train Loss: 723.8597142948306  Validation Loss: 3400.907984988221
Epoch: 780  Train Loss: 1001.4425309741777  Validation Loss: 3522.3123819462744
Epoch: 790  Train Loss: 941.7840083995403  Validation Loss: 3389.954667576464
Epoch: 800  Train Loss: 1044.8311388190677  Validation Loss: 4009.4263272538165
Epoch: 810  Train Loss: 1017.0435050098577  Validation Loss: 2808.0177797729807
Epoch: 820  Train Loss: 1004.2632764826138  Validation Loss: 2520.2491963778834
Epoch: 830  Train Loss: 1538.097210543482  Validation Loss: 4836.2856709441985
Epoch: 840  Train Loss: 1583.1552330530797  Validation Loss: 2519.586348126271
Epoch: 850  Train Loss: 784.977536295651  Validation Loss: 2878.8358107899758
Epoch: 860  Train Loss: 801.8006123599183  Validation Loss: 2798.1088008590973
Epoch: 870  Train Loss: 899.4893233087579  Validation Loss: 3040.543566493028
Epoch: 880  Train Loss: 847.1639127727707  Validation Loss: 2767.971223824941
Epoch: 890  Train Loss: 905.7754772463977  Validation Loss: 2533.955727530455
Epoch: 900  Train Loss: 890.3635594144295  Validation Loss: 2312.453396939741
Epoch: 910  Train Loss: 915.4431959242434  Validation Loss: 5255.273294714424
Epoch: 920  Train Loss: 1082.0194551822478  Validation Loss: 2419.7864222263115
Epoch: 930  Train Loss: 837.409334014708  Validation Loss: 2628.7846391906874
Epoch: 940  Train Loss: 664.0469330643047  Validation Loss: 2387.621375155882
Epoch: 950  Train Loss: 884.8984686172053  Validation Loss: 2252.10382120092
Epoch: 960  Train Loss: 1768.397955359691  Validation Loss: 10811.618815272317
Epoch: 970  Train Loss: 1041.8485827853544  Validation Loss: 3103.0022191587227
Epoch: 980  Train Loss: 702.0407448974764  Validation Loss: 2669.4534148809043
Epoch: 990  Train Loss: 1013.4548977194581  Validation Loss: 2541.628803668739
torch.Size([8100])
torch.Size([8100])
100
100
Average Test Loss (MSE) tensor(2540.2197, device='cuda:0', grad_fn=<DivBackward0>)